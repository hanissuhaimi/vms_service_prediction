{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T02:56:24.792117Z",
     "start_time": "2025-05-27T02:56:21.407619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, \n",
    "                           precision_recall_fscore_support)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "376640dd3141dc7c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:01:59.227404Z",
     "start_time": "2025-05-27T02:58:55.120074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create output folder\n",
    "OUTPUT_DIR = \"model_training_output\"\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VMS MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load data\n",
    "print(\"\\n1. LOADING DATA\")\n",
    "print(\"-\" * 40)\n",
    "try:\n",
    "    df = pd.read_excel('data/Cleaned_ServiceRequest.xlsx')\n",
    "    print(f\"âœ“ Loaded data with shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        df = pd.read_excel('data/Cleaned_ServiceRequest.xlsx')\n",
    "        print(f\"âœ“ Loaded data with shape: {df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"âœ— Error: Could not find data file\")\n",
    "        exit(1)"
   ],
   "id": "b2e8837382353304",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VMS MODEL TRAINING\n",
      "================================================================================\n",
      "\n",
      "1. LOADING DATA\n",
      "----------------------------------------\n",
      "âœ“ Loaded data with shape: (511645, 37)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:02:18.646321Z",
     "start_time": "2025-05-27T03:02:18.633229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define important features\n",
    "core_features = [\n",
    "    'Priority', 'service_count',\n",
    "    'Building_encoded', 'Vehicle_encoded', 'Status_encoded', 'MrType_encoded'\n",
    "]\n",
    "\n",
    "# Add time features\n",
    "time_features = ['request_day_of_week', 'request_month', 'request_hour']\n",
    "high_impact_features = ['response_days', 'Odometer']\n",
    "\n",
    "# Build feature list\n",
    "features = [f for f in core_features if f in df.columns]\n",
    "features.extend([f for f in time_features if f in df.columns])\n",
    "features.extend([f for f in high_impact_features if f in df.columns])\n",
    "\n",
    "text_feature = 'Description' if 'Description' in df.columns else None\n",
    "target = 'maintenance_category'\n",
    "\n",
    "print(f\"Using features: {features}\")\n",
    "print(f\"Text feature: {text_feature}\")"
   ],
   "id": "423e38e0c8f97e52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using features: ['Priority', 'service_count', 'Building_encoded', 'Vehicle_encoded', 'Status_encoded', 'MrType_encoded', 'request_day_of_week', 'request_month', 'request_hour', 'response_days', 'Odometer']\n",
      "Text feature: Description\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:02:23.228994Z",
     "start_time": "2025-05-27T03:02:22.960874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check data\n",
    "if target not in df.columns:\n",
    "    print(f\"âœ— Error: Target variable '{target}' not found\")\n",
    "    exit(1)\n",
    "\n",
    "df_clean = df.dropna(subset=[target]).copy()\n",
    "print(f\"Dataset shape after cleaning: {df_clean.shape}\")"
   ],
   "id": "1e4bb92a871ea6d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape after cleaning: (511645, 37)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:02:31.987122Z",
     "start_time": "2025-05-27T03:02:31.835164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add useful features\n",
    "print(\"\\n2. FEATURE ENGINEERING\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def create_features(df):\n",
    "    \"\"\"Add important features\"\"\"\n",
    "    df_enhanced = df.copy()\n",
    "\n",
    "    # Weekend\n",
    "    if 'request_day_of_week' in df.columns:\n",
    "        df_enhanced['is_weekend'] = (df['request_day_of_week'] >= 5).astype(int)\n",
    "\n",
    "    # Business hours\n",
    "    if 'request_hour' in df.columns:\n",
    "        df_enhanced['is_business_hours'] = ((df['request_hour'] >= 8) &\n",
    "                                          (df['request_hour'] <= 17)).astype(int)\n",
    "\n",
    "    # High maintenance vehicle\n",
    "    if 'service_count' in df.columns:\n",
    "        service_threshold = df['service_count'].quantile(0.75)\n",
    "        df_enhanced['high_maintenance_vehicle'] = (df['service_count'] >= service_threshold).astype(int)\n",
    "\n",
    "    return df_enhanced\n",
    "\n",
    "df_enhanced = create_features(df_clean)"
   ],
   "id": "85a93dfe7d27faeb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. FEATURE ENGINEERING\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:02:48.598154Z",
     "start_time": "2025-05-27T03:02:48.584603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add new features to list\n",
    "new_features = ['is_weekend', 'is_business_hours', 'high_maintenance_vehicle']\n",
    "new_features = [f for f in new_features if f in df_enhanced.columns]\n",
    "features.extend(new_features)\n",
    "\n",
    "print(f\"âœ“ Added {len(new_features)} new features\")\n",
    "print(f\"Total features: {len(features)}\")\n",
    "\n",
    "print(\"\\n3. DATA PREPROCESSING\")\n",
    "print(\"-\" * 40)"
   ],
   "id": "ddfeea2ab797971",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Added 3 new features\n",
      "Total features: 14\n",
      "\n",
      "3. DATA PREPROCESSING\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:02:53.877574Z",
     "start_time": "2025-05-27T03:02:53.861940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split features by type\n",
    "numerical_features = []\n",
    "categorical_features = []\n",
    "\n",
    "for f in features:\n",
    "    if df_enhanced[f].dtype in ['int64', 'float64']:\n",
    "        numerical_features.append(f)\n",
    "    else:\n",
    "        categorical_features.append(f)\n",
    "\n",
    "print(f\"Numerical: {len(numerical_features)}, Categorical: {len(categorical_features)}\")"
   ],
   "id": "db25627dd46ed1f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical: 14, Categorical: 0\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:02:58.235327Z",
     "start_time": "2025-05-27T03:02:57.491094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare data\n",
    "X_numerical = df_enhanced[numerical_features] if numerical_features else pd.DataFrame()\n",
    "X_categorical = df_enhanced[categorical_features] if categorical_features else pd.DataFrame()\n",
    "X_text = df_enhanced[text_feature] if text_feature else pd.Series('', index=df_enhanced.index)\n",
    "y = df_enhanced[target]\n",
    "\n",
    "# Encode target\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train_idx, X_test_idx = train_test_split(\n",
    "    range(len(df_enhanced)),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(X_train_idx):,}, Test: {len(X_test_idx):,}\")"
   ],
   "id": "5e1af562b00cc09f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 409,316, Test: 102,329\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:03:12.401350Z",
     "start_time": "2025-05-27T03:03:02.793749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Process features\n",
    "processed_features = []\n",
    "feature_names = []\n",
    "\n",
    "# Numbers\n",
    "if not X_numerical.empty:\n",
    "    print(\"Processing numerical features...\")\n",
    "    numerical_imputer = SimpleImputer(strategy='median')\n",
    "    numerical_scaler = StandardScaler()\n",
    "\n",
    "    X_num_train = numerical_imputer.fit_transform(X_numerical.iloc[X_train_idx])\n",
    "    X_num_test = numerical_imputer.transform(X_numerical.iloc[X_test_idx])\n",
    "\n",
    "    X_num_train = numerical_scaler.fit_transform(X_num_train)\n",
    "    X_num_test = numerical_scaler.transform(X_num_test)\n",
    "\n",
    "    processed_features.append(('numerical', X_num_train, X_num_test))\n",
    "    feature_names.extend(numerical_features)\n",
    "\n",
    "# Categories\n",
    "if not X_categorical.empty:\n",
    "    print(\"Processing categorical features...\")\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    X_cat_train = categorical_imputer.fit_transform(X_categorical.iloc[X_train_idx])\n",
    "    X_cat_test = categorical_imputer.transform(X_categorical.iloc[X_test_idx])\n",
    "\n",
    "    processed_features.append(('categorical', X_cat_train, X_cat_test))\n",
    "    feature_names.extend(categorical_features)\n",
    "\n",
    "# Text\n",
    "tfidf = None\n",
    "if text_feature and text_feature in df_enhanced.columns:\n",
    "    print(\"Processing text features...\")\n",
    "    X_text_clean = X_text.fillna('').astype(str)\n",
    "\n",
    "    # TF-IDF for text\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=100,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.95\n",
    "    )\n",
    "\n",
    "    X_text_train = tfidf.fit_transform(X_text_clean.iloc[X_train_idx])\n",
    "    X_text_test = tfidf.transform(X_text_clean.iloc[X_test_idx])\n",
    "\n",
    "    processed_features.append(('text', X_text_train, X_text_test))\n",
    "    text_feature_names = [f'text_{f}' for f in tfidf.get_feature_names_out()]\n",
    "    feature_names.extend(text_feature_names)\n",
    "\n",
    "# Combine features\n",
    "if processed_features:\n",
    "    train_matrices = []\n",
    "    test_matrices = []\n",
    "\n",
    "    for feature_type, train_data, test_data in processed_features:\n",
    "        train_matrices.append(csr_matrix(train_data))\n",
    "        test_matrices.append(csr_matrix(test_data))\n",
    "\n",
    "    X_train_combined = hstack(train_matrices)\n",
    "    X_test_combined = hstack(test_matrices)\n",
    "    y_train = y_encoded[X_train_idx]\n",
    "    y_test = y_encoded[X_test_idx]\n",
    "else:\n",
    "    print(\"âœ— Error: No features to process!\")\n",
    "    exit(1)\n",
    "\n",
    "# Feature selection\n",
    "if X_train_combined.shape[1] > 50:\n",
    "    print(\"Applying feature selection...\")\n",
    "    selector = SelectKBest(f_classif, k=min(50, X_train_combined.shape[1]))\n",
    "    X_train_final = selector.fit_transform(X_train_combined, y_train)\n",
    "    X_test_final = selector.transform(X_test_combined)\n",
    "\n",
    "    selected_indices = selector.get_support()\n",
    "    final_feature_names = [feature_names[i] for i in range(len(feature_names)) if i < len(selected_indices) and selected_indices[i]]\n",
    "    print(f\"  âœ“ Selected {X_train_final.shape[1]} features\")\n",
    "else:\n",
    "    X_train_final = X_train_combined\n",
    "    X_test_final = X_test_combined\n",
    "    final_feature_names = feature_names\n",
    "\n",
    "print(f\"Final feature matrix: {X_train_final.shape}\")\n",
    "\n",
    "print(\"\\n4. MODEL TRAINING\")\n",
    "print(\"-\" * 40)"
   ],
   "id": "5965881ea5f178fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing numerical features...\n",
      "Processing text features...\n",
      "Applying feature selection...\n",
      "  âœ“ Selected 50 features\n",
      "Final feature matrix: (409316, 50)\n",
      "\n",
      "4. MODEL TRAINING\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:04:10.203753Z",
     "start_time": "2025-05-27T03:04:10.064012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))"
   ],
   "id": "b8894211b1ba647c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:07:35.739817Z",
     "start_time": "2025-05-27T03:04:12.499987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train model\n",
    "print(\"Training Gradient Boosting model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=80,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=8,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10\n",
    ")\n",
    "\n",
    "# Train model\n",
    "gb_model.fit(X_train_final, y_train)\n",
    "training_time = time.time() - start_time"
   ],
   "id": "7cd59847554cbbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Gradient Boosting model...\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:07:45.106255Z",
     "start_time": "2025-05-27T03:07:44.250395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test model\n",
    "y_pred = gb_model.predict(X_test_final)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"âœ“ Training completed in {training_time:.2f}s\")\n",
    "print(f\"âœ“ Test Accuracy: {accuracy:.3f}\")\n",
    "print(f\"âœ“ Precision: {precision:.3f}\")\n",
    "print(f\"âœ“ Recall: {recall:.3f}\")\n",
    "print(f\"âœ“ F1-Score: {f1:.3f}\")"
   ],
   "id": "cf600f7468177e9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Training completed in 203.23s\n",
      "âœ“ Test Accuracy: 0.915\n",
      "âœ“ Precision: 0.918\n",
      "âœ“ Recall: 0.915\n",
      "âœ“ F1-Score: 0.912\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:07:50.455019Z",
     "start_time": "2025-05-27T03:07:50.393489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use the trained model directly\n",
    "final_model = gb_model\n",
    "final_predictions = y_pred\n",
    "final_accuracy = accuracy\n",
    "\n",
    "print(\"\\n5. EVALUATION AND SAVING\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Show results\n",
    "target_names = le_target.classes_\n",
    "class_report = classification_report(y_test, final_predictions, target_names=target_names)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ],
   "id": "b31101a093d5e85a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. EVALUATION AND SAVING\n",
      "----------------------------------------\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  air_system       0.90      0.82      0.86      5336\n",
      "        body       0.81      0.75      0.78      1001\n",
      "brake_system       0.92      0.79      0.85      8818\n",
      "    cleaning       1.00      1.00      1.00     26404\n",
      "  electrical       0.97      0.89      0.93      4370\n",
      "      engine       0.82      0.80      0.81      4515\n",
      "   hydraulic       0.06      0.01      0.02       170\n",
      "  mechanical       0.88      0.80      0.84      4301\n",
      "       other       0.79      0.95      0.86     20618\n",
      "     service       0.97      0.83      0.90      1715\n",
      "        tire       0.98      0.99      0.99     21840\n",
      "     unknown       0.85      0.49      0.62      3241\n",
      "\n",
      "    accuracy                           0.91    102329\n",
      "   macro avg       0.83      0.76      0.79    102329\n",
      "weighted avg       0.92      0.91      0.91    102329\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:08:08.190930Z",
     "start_time": "2025-05-27T03:08:06.126783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, final_predictions)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title('Confusion Matrix - Gradient Boosting')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"âœ“ Saved confusion matrix\")"
   ],
   "id": "416e94da0d11fafc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved confusion matrix\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:08:11.686162Z",
     "start_time": "2025-05-27T03:08:11.117145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature importance\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    importances = final_model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': final_feature_names[:len(importances)],\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    feature_importance_df.to_csv(os.path.join(OUTPUT_DIR, 'feature_importance.csv'), index=False)\n",
    "\n",
    "    # Plot top 15 features\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features = feature_importance_df.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'].values)\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'].values)\n",
    "    plt.title('Top 15 Feature Importances')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'feature_importance.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"âœ“ Saved feature importance\")"
   ],
   "id": "7945a5e947e69786",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved feature importance\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:08:15.141460Z",
     "start_time": "2025-05-27T03:08:15.097112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save model\n",
    "model_objects = {\n",
    "    'final_model': final_model,\n",
    "    'model_type': 'Gradient Boosting',\n",
    "    'numerical_features': numerical_features,\n",
    "    'categorical_features': categorical_features,\n",
    "    'text_feature': text_feature,\n",
    "    'feature_names': final_feature_names,\n",
    "    'numerical_imputer': numerical_imputer if not X_numerical.empty else None,\n",
    "    'numerical_scaler': numerical_scaler if not X_numerical.empty else None,\n",
    "    'categorical_imputer': categorical_imputer if not X_categorical.empty else None,\n",
    "    'tfidf': tfidf,\n",
    "    'feature_selector': selector if 'selector' in locals() else None,\n",
    "    'label_encoder': le_target,\n",
    "    'classes': target_names,\n",
    "    'model_performance': {\n",
    "        'accuracy': final_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'training_time': training_time\n",
    "    },\n",
    "    'training_metadata': {\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'training_samples': len(X_train_idx),\n",
    "        'test_samples': len(X_test_idx),\n",
    "        'n_features': X_train_final.shape[1],\n",
    "        'n_classes': len(target_names),\n",
    "        'optimization_applied': False\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save model file\n",
    "model_filename = os.path.join(OUTPUT_DIR, 'maintenance_prediction_model.pkl')\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(model_objects, f)\n",
    "\n",
    "print(f\"âœ“ Saved model to: {model_filename}\")"
   ],
   "id": "2e2ec1844f41bfa2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved model to: model_training_output\\maintenance_prediction_model.pkl\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T03:08:24.756413Z",
     "start_time": "2025-05-27T03:08:24.730178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create summary report\n",
    "summary = f\"\"\"\n",
    "VMS MODEL TRAINING SUMMARY\n",
    "=========================\n",
    "\n",
    "Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Training Time: {training_time:.2f} seconds\n",
    "Final Accuracy: {final_accuracy:.3f}\n",
    "\n",
    "Dataset:\n",
    "- Training samples: {len(X_train_idx):,}\n",
    "- Test samples: {len(X_test_idx):,}\n",
    "- Features used: {X_train_final.shape[1]}\n",
    "- Classes: {len(target_names)}\n",
    "\n",
    "Performance:\n",
    "- Accuracy: {final_accuracy:.1%}\n",
    "- Precision: {precision:.1%}\n",
    "- Recall: {recall:.1%}\n",
    "- F1-Score: {f1:.1%}\n",
    "\n",
    "Top 5 Features:\n",
    "\"\"\"\n",
    "\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    for i, (_, row) in enumerate(feature_importance_df.head(5).iterrows()):\n",
    "        summary += f\"  {i+1}. {row['feature']}: {row['importance']:.4f}\\n\"\n",
    "\n",
    "summary += f\"\"\"\n",
    "Files Generated:\n",
    "- maintenance_prediction_model.pkl\n",
    "- confusion_matrix.png\n",
    "- feature_importance.csv\n",
    "- feature_importance.png\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'model_summary.txt'), 'w') as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL TRAINING COMPLETED! ðŸš€\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ðŸŽ¯ Model: Gradient Boosting\")\n",
    "print(f\"ðŸŽ¯ Parameters: n_estimators=80, learning_rate=0.1, max_depth=8\")\n",
    "print(f\"ðŸŽ¯ Accuracy: {final_accuracy:.1%}\")\n",
    "print(f\"ðŸŽ¯ Training Time: {training_time:.2f} seconds\")\n",
    "print(f\"ðŸŽ¯ Features: {X_train_final.shape[1]}\")\n",
    "print(f\"ðŸŽ¯ Samples: {len(X_train_idx):,}\")\n",
    "\n",
    "print(f\"\\nâ±ï¸  TOTAL TRAINING TIME: {training_time:.2f} seconds\")\n",
    "print(f\"ðŸš€ Model ready for deployment!\")\n",
    "print(f\"ðŸ“ Saved as: {model_filename}\")"
   ],
   "id": "18b5523065bd744e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL TRAINING COMPLETED! ðŸš€\n",
      "================================================================================\n",
      "ðŸŽ¯ Model: Gradient Boosting\n",
      "ðŸŽ¯ Parameters: n_estimators=80, learning_rate=0.1, max_depth=8\n",
      "ðŸŽ¯ Accuracy: 91.5%\n",
      "ðŸŽ¯ Training Time: 203.23 seconds\n",
      "ðŸŽ¯ Features: 50\n",
      "ðŸŽ¯ Samples: 409,316\n",
      "\n",
      "â±ï¸  TOTAL TRAINING TIME: 203.23 seconds\n",
      "ðŸš€ Model ready for deployment!\n",
      "ðŸ“ Saved as: model_training_output\\maintenance_prediction_model.pkl\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
